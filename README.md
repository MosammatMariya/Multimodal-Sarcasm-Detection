This  introduces a system for detecting sarcasm that uses audio, text, and video data to improve the accuracy and dependability of identifying sarcasm in interactions between humans and machines. The system utilizes YAMNet for analyzing audio, DistilBERT for extracting text features, and MobileNetV2 for processing video frames. Features from these different types of data are combined using a concatenation method followed by an attention mechanism, resulting in a binary classification output through a series of fully connected layers. Tested on a specially selected dataset of sarcastic and non-sarcastic clips, the model reaches an accuracy of about 65.2%, after being trained for up to 300 epochs with L2 regularization and early stopping. This research highlights the potential of integrating various data types to capture intricate sarcastic signals, with uses in social media analysis and conversational AI.
