{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8735ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio feature vector size: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Initialize Wav2Vec2 processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec2_model.to(device)\n",
    "\n",
    "# Function to load and preprocess the audio file\n",
    "def load_audio(audio_path):\n",
    "    # Load the audio file (using pydub to handle .wav format)\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    # Convert to mono and set the sample rate (Wav2Vec2 expects 16kHz)\n",
    "    audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "    # Export to raw audio data\n",
    "    raw_audio = audio.get_array_of_samples()\n",
    "    return torch.tensor(raw_audio).float()\n",
    "\n",
    "# Function to extract features using Wav2Vec2\n",
    "def extract_audio_features(audio_path):\n",
    "    audio_data = load_audio(audio_path)\n",
    "    # Tokenize the audio data\n",
    "    inputs = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000)\n",
    "    # Move inputs to the same device as the model (GPU or CPU)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Extract features using Wav2Vec2 model\n",
    "        outputs = wav2vec2_model(**inputs)\n",
    "        # Extracting the embeddings (mean pooling across time steps)\n",
    "        audio_features = outputs.last_hidden_state.mean(dim=1)  # Output size: (1, 768)\n",
    "    \n",
    "    # Debugging: Checking the output size\n",
    "    print(f\"Audio feature vector size: {audio_features.size()}\")\n",
    "    \n",
    "    return audio_features\n",
    "\n",
    "# Test the audio feature extraction\n",
    "audio_file_path = \"T:/0 datasets/dum4/audios/sarcastic/sea1_ep1_sc4_utt3.wav\"\n",
    "audio_features = extract_audio_features(audio_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e980760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature vector size: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize TinyBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "tinybert_model = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tinybert_model.to(device)\n",
    "\n",
    "# Function to load and preprocess the text file\n",
    "def load_text(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Function to extract features using TinyBERT\n",
    "def extract_text_features(text_path):\n",
    "    text = load_text(text_path)\n",
    "    # Tokenize the text (with padding and truncation to a fixed length)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    # Move inputs to the same device as the model (GPU or CPU)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Extract features using TinyBERT model\n",
    "        outputs = tinybert_model(**inputs)\n",
    "        # Extract the [CLS] token (first token) representation\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]  # Output size: (1, 312)\n",
    "        # Project to 768 dimensions\n",
    "        text_features = torch.nn.Linear(312, 768).to(device)(text_features)  # Output size: (1, 768)\n",
    "    \n",
    "    # Debugging: Checking the output size\n",
    "    print(f\"Text feature vector size: {text_features.size()}\")\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "# Test the text feature extraction\n",
    "text_file_path = \"T:/0 datasets/dum4/text/sarcastic/sea1_ep1_sc4_utt3.txt\"\n",
    "text_features = extract_text_features(text_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a6232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rifat\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rifat\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 23.976042590949422, Total frames: 67\n",
      "Video feature vector size: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Initialize ResNet-18 and move to GPU if available\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18 = torch.nn.Sequential(*list(resnet18.children())[:-1])  # Remove the final classification layer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18.to(device)\n",
    "\n",
    "# Preprocessing transformations for video frames (resize and normalize)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert the frame to PIL image\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 pixels\n",
    "    transforms.ToTensor(),  # Convert to Tensor format\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Function to extract 1 frame per second from the video\n",
    "def extract_video_frames(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)  # Get frames per second (fps) of the video\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames in the video\n",
    "    \n",
    "    print(f\"FPS: {fps}, Total frames: {total_frames}\")\n",
    "\n",
    "    frame_count = 0\n",
    "    frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # Stop if no more frames are available\n",
    "        \n",
    "        # Extract 1 frame per second based on FPS\n",
    "        if frame_count % int(fps) == 0:  # Extract 1 frame every second\n",
    "            frames.append(frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    video.release()\n",
    "    return frames\n",
    "\n",
    "# Function to extract features using ResNet-18\n",
    "def extract_video_features(video_path):\n",
    "    frames = extract_video_frames(video_path)  # Extract 1 frame per second\n",
    "    frame_features = []\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = preprocess(frame)  # Resize and normalize the frame\n",
    "        frame = frame.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            feature = resnet18(frame)  # Extract features using ResNet-18\n",
    "            feature = feature.view(feature.size(0), -1)  # Flatten the output to 1D\n",
    "            frame_features.append(feature)\n",
    "\n",
    "    # Take the mean of the frame features (to get a fixed-size feature vector)\n",
    "    video_features = torch.mean(torch.stack(frame_features), dim=0)\n",
    "    # Project to 768 dimensions and move the projection to the GPU\n",
    "    video_features = torch.nn.Linear(512, 768).to(device)(video_features)  # Project from 512 to 768 dimensions\n",
    "    \n",
    "    # Debugging: Checking the output size\n",
    "    print(f\"Video feature vector size: {video_features.size()}\")\n",
    "    \n",
    "    return video_features\n",
    "\n",
    "# Test the video feature extraction\n",
    "video_file_path = \"T:/0 datasets/dum4/vids/sarcastic/sea1_ep1_sc4_utt3.mp4\"\n",
    "video_features = extract_video_features(video_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acb3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Features shape: torch.Size([32, 2304])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming audio_features, text_features, and video_features are already obtained\n",
    "# Move tensors to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Concatenate the features along the feature dimension (dim=1)\n",
    "def concatenate_features(audio_features, text_features, video_features):\n",
    "    # Ensure that all tensors are on the same device\n",
    "    audio_features = audio_features.to(device)\n",
    "    text_features = text_features.to(device)\n",
    "    video_features = video_features.to(device)\n",
    "\n",
    "    # Concatenate the features\n",
    "    features = torch.cat([audio_features, text_features, video_features], dim=1)\n",
    "    \n",
    "    # Debugging: Check the shape after concatenation\n",
    "    print(f\"Concatenated Features shape: {features.size()}\")  # Should print [batch_size, 2304]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example for concatenation (batch_size=32 in this case)\n",
    "audio_features = torch.randn(32, 768).to(device)  # Example audio features\n",
    "text_features = torch.randn(32, 768).to(device)   # Example text features\n",
    "video_features = torch.randn(32, 768).to(device)  # Example video features\n",
    "\n",
    "# Concatenate the features\n",
    "features = concatenate_features(audio_features, text_features, video_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36336175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalSarcasmDetectionModel(\n",
      "  (fc1): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (layer_norm): LayerNorm((2304,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultimodalSarcasmDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalSarcasmDetectionModel, self).__init__()\n",
    "        \n",
    "        # Define the first fully connected layer (from 2304 input features to 1024 output features)\n",
    "        self.fc1 = nn.Linear(2304, 1024)  # First fully connected layer\n",
    "        self.relu1 = nn.ReLU()  # ReLU activation\n",
    "        \n",
    "        # Additional hidden layers\n",
    "        self.fc2 = nn.Linear(1024, 512)  # Second fully connected layer\n",
    "        self.relu2 = nn.ReLU()  # ReLU activation\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)  # Third fully connected layer\n",
    "        self.relu3 = nn.ReLU()  # ReLU activation\n",
    "        \n",
    "        # Output layer (binary classification)\n",
    "        self.fc4 = nn.Linear(256, 1)  # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        \n",
    "        # Layer normalization (normalize the concatenated feature vector)\n",
    "        self.layer_norm = nn.LayerNorm(2304)  # Normalize the concatenated feature vector\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        - Accept a single concatenated feature vector.\n",
    "        - Normalize the concatenated features.\n",
    "        - Pass through the fully connected layers.\n",
    "        \"\"\"\n",
    "        # Normalize the concatenated feature vector\n",
    "        features = self.layer_norm(features)\n",
    "        \n",
    "        # Pass through the first fully connected layer (fc1)\n",
    "        x = self.fc1(features)\n",
    "        x = self.relu1(x)  # Apply ReLU activation\n",
    "        \n",
    "        # Pass through the second fully connected layer (fc2)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)  # Apply ReLU activation\n",
    "        \n",
    "        # Pass through the third fully connected layer (fc3)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)  # Apply ReLU activation\n",
    "        \n",
    "        # Output layer: This is the final layer that gives the classification score (one value per sample)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)  # Sigmoid for binary classification\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Device setup: Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultimodalSarcasmDetectionModel()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Print the model architecture (optional)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b7b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 768]) torch.Size([64, 1, 768]) torch.Size([64, 1, 768]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Wav2Vec2Processor, AutoTokenizer\n",
    "from pydub import AudioSegment\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from transformers import Wav2Vec2Model, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultimodalSarcasmDataset(Dataset):\n",
    "    def __init__(self, audio_paths, text_paths, video_paths, labels, audio_processor, text_tokenizer, video_transform, device):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.text_paths = text_paths\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.audio_processor = audio_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.video_transform = video_transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        text_path = self.text_paths[idx]\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Extract audio features\n",
    "        audio_features = self.extract_audio_features(audio_path)\n",
    "        \n",
    "        # Extract text features\n",
    "        text_features = self.extract_text_features(text_path)\n",
    "\n",
    "        # Extract video features\n",
    "        video_features = self.extract_video_features(video_path)\n",
    "        \n",
    "        # Move features to the same device as the model\n",
    "        audio_features = audio_features.to(self.device)\n",
    "        text_features = text_features.to(self.device)\n",
    "        video_features = video_features.to(self.device)\n",
    "\n",
    "        # Debugging: Checking dimensions of extracted features\n",
    "        #print(f\"Audio features size: {audio_features.size()}\")\n",
    "        #print(f\"Text features size: {text_features.size()}\")\n",
    "        #print(f\"Video features size: {video_features.size()}\")\n",
    "        \n",
    "        # Return the features and label\n",
    "        return audio_features, text_features, video_features, torch.tensor(label).to(self.device)\n",
    "\n",
    "    def extract_audio_features(self, audio_path):\n",
    "        audio = AudioSegment.from_wav(audio_path)\n",
    "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "        audio_data = torch.tensor(audio.get_array_of_samples()).float()\n",
    "\n",
    "        # Move the audio tensor to the same device as the model\n",
    "        audio_data = audio_data.to(self.device)\n",
    "\n",
    "        # Tokenize the audio data and ensure it is on the same device as the model\n",
    "        inputs = self.audio_processor(audio_data, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}  # Move all inputs to the device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Extract features using wav2vec2_model\n",
    "            outputs = wav2vec2_model(**inputs)\n",
    "            audio_features = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "        #print(f\"Extracted audio features size: {audio_features.size()}\")  # Debugging\n",
    "        return audio_features\n",
    "\n",
    "    def extract_text_features(self, text_path):\n",
    "        with open(text_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        inputs = self.text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}  # Ensure inputs are on the same device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = tinybert_model(**inputs)\n",
    "            text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            text_features = torch.nn.Linear(312, 768).to(self.device)(text_features)  # Project to 768\n",
    "        #print(f\"Extracted text features size: {text_features.size()}\")  # Debugging\n",
    "        return text_features\n",
    "\n",
    "    def extract_video_features(self, video_path):\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "\n",
    "        frame_features = []\n",
    "        for frame in frames:\n",
    "            frame = self.video_transform(frame)  # Resize and normalize the frame\n",
    "            frame = frame.unsqueeze(0).to(self.device)  # Add batch dimension and move to GPU\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                feature = resnet18(frame)  # Extract features\n",
    "                feature = feature.view(feature.size(0), -1)  # Flatten\n",
    "                frame_features.append(feature)\n",
    "\n",
    "        video_features = torch.mean(torch.stack(frame_features), dim=0)\n",
    "        video_features = torch.nn.Linear(512, 768).to(self.device)(video_features)  # Project to 768\n",
    "        #print(f\"Extracted video features size: {video_features.size()}\")  # Debugging\n",
    "        return video_features\n",
    "\n",
    "# Example paths (replace with actual paths from your dataset)\n",
    "audio_sarcastic_folder = \"T:/0 datasets/dum4/audios/sarcastic\"\n",
    "audio_nonsarcastic_folder = \"T:/0 datasets/dum4/audios/nonsarcastic\"\n",
    "text_sarcastic_folder = \"T:/0 datasets/dum4/text/sarcastic\"\n",
    "text_nonsarcastic_folder = \"T:/0 datasets/dum4/text/nonsarcastic\"\n",
    "video_sarcastic_folder = \"T:/0 datasets/dum4/vids/sarcastic\"\n",
    "video_nonsarcastic_folder = \"T:/0 datasets/dum4/vids/nonsarcastic\"\n",
    "\n",
    "audio_paths = []\n",
    "text_paths = []\n",
    "video_paths = []\n",
    "labels = []\n",
    "\n",
    "# Populate the paths for sarcastic data (1) and nonsarcastic data (0)\n",
    "for audio_file in os.listdir(audio_sarcastic_folder):\n",
    "    audio_paths.append(os.path.join(audio_sarcastic_folder, audio_file))\n",
    "    text_paths.append(os.path.join(text_sarcastic_folder, audio_file.replace(\".wav\", \".txt\")))\n",
    "    video_paths.append(os.path.join(video_sarcastic_folder, audio_file.replace(\".wav\", \".mp4\")))\n",
    "    labels.append(1)\n",
    "\n",
    "for audio_file in os.listdir(audio_nonsarcastic_folder):\n",
    "    audio_paths.append(os.path.join(audio_nonsarcastic_folder, audio_file))\n",
    "    text_paths.append(os.path.join(text_nonsarcastic_folder, audio_file.replace(\".wav\", \".txt\")))\n",
    "    video_paths.append(os.path.join(video_nonsarcastic_folder, audio_file.replace(\".wav\", \".mp4\")))\n",
    "    labels.append(0)\n",
    "\n",
    "# Device setup: Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models and processors\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "resnet18 = models.resnet18(pretrained=True).to(device)\n",
    "resnet18 = torch.nn.Sequential(*list(resnet18.children())[:-1])  # Remove final classification layer\n",
    "\n",
    "# Instantiate models for audio and text\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "tinybert_model = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\").to(device)\n",
    "\n",
    "# Preprocessing transformations for video frames (resize and normalize)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert the frame to PIL image\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 pixels\n",
    "    transforms.ToTensor(),  # Convert to Tensor format\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = MultimodalSarcasmDataset(\n",
    "    audio_paths=audio_paths,\n",
    "    text_paths=text_paths,\n",
    "    video_paths=video_paths,\n",
    "    labels=labels,\n",
    "    audio_processor=processor,\n",
    "    text_tokenizer=tokenizer,\n",
    "    video_transform=preprocess,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create the DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Testing the data loading (just a few iterations)\n",
    "for batch in dataloader:\n",
    "    audio_features, text_features, video_features, labels = batch\n",
    "    print(audio_features.size(), text_features.size(), video_features.size(), labels.size())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d2b7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Loss: 0.6949, Accuracy: 0.5008, F1-Score: 0.0033, ROC-AUC: 0.5008\n",
      "Epoch 2/5\n",
      "Loss: 0.6931, Accuracy: 0.5000, F1-Score: 0.0000, ROC-AUC: 0.5000\n",
      "Epoch 3/5\n",
      "Loss: 0.6931, Accuracy: 0.5000, F1-Score: 0.0000, ROC-AUC: 0.5000\n",
      "Epoch 4/5\n",
      "Loss: 0.6931, Accuracy: 0.5000, F1-Score: 0.0000, ROC-AUC: 0.5000\n",
      "Epoch 5/5\n",
      "Loss: 0.6931, Accuracy: 0.5000, F1-Score: 0.0000, ROC-AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Instantiate the model and move it to the appropriate device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalSarcasmDetectionModel().to(device)\n",
    "\n",
    "# Define the loss function (Binary Cross-Entropy Loss)\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)  # Move loss function to GPU\n",
    "\n",
    "# Define the optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function with GPU support\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for batch_idx, (audio_features, text_features, video_features, labels) in enumerate(dataloader):\n",
    "            # Move data to the GPU\n",
    "            audio_features = audio_features.to(device)\n",
    "            text_features = text_features.to(device)\n",
    "            video_features = video_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Squeeze the features to remove the singleton dimension\n",
    "            audio_features = audio_features.squeeze(1)  # Shape: [batch_size, 768]\n",
    "            text_features = text_features.squeeze(1)    # Shape: [batch_size, 768]\n",
    "            video_features = video_features.squeeze(1)  # Shape: [batch_size, 768]\n",
    "\n",
    "            # Concatenate the features along the feature dimension (dim=1)\n",
    "            features = torch.cat([audio_features, text_features, video_features], dim=1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)  # Forward pass through the model\n",
    "\n",
    "            # Ensure labels have the same shape as outputs\n",
    "            labels = labels.unsqueeze(1)  # Change labels to shape [batch_size, 1]\n",
    "            labels = labels.float()  # Convert labels to Float type (required by BCEWithLogitsLoss)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "            \n",
    "            # Track running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Convert output to predicted class (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend((outputs > 0.5).cpu().numpy())  # Convert probability to binary class\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model (architecture + weights) using .pt extension\n",
    "torch.save(model, \"T:/000 Models/dum4full_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7006a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3021313\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Example: Print the number of parameters in your model\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Total number of parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920472be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1.weight, Parameters: 2359296\n",
      "Layer: fc1.bias, Parameters: 1024\n",
      "Layer: fc2.weight, Parameters: 524288\n",
      "Layer: fc2.bias, Parameters: 512\n",
      "Layer: fc3.weight, Parameters: 131072\n",
      "Layer: fc3.bias, Parameters: 256\n",
      "Layer: fc4.weight, Parameters: 256\n",
      "Layer: fc4.bias, Parameters: 1\n",
      "Layer: layer_norm.weight, Parameters: 2304\n",
      "Layer: layer_norm.bias, Parameters: 2304\n"
     ]
    }
   ],
   "source": [
    "def count_parameters_by_layer(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name}, Parameters: {param.numel()}\")\n",
    "        \n",
    "# Example: Print the number of parameters in each layer\n",
    "count_parameters_by_layer(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdb061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
